{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This cell loads all previous model and create a stacker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking Lasso trainin set size:  (1458,)\n",
      "Lasso RMSE:  0.112229412799\n",
      "\n",
      "Checking Ridge trainin set size:  (1458,)\n",
      "Ridge RMSE:  0.117605942387\n",
      "\n",
      "Checking XGB training set size:  (1458,)\n",
      "XGB RMSE:  0.117635216797\n",
      "\n",
      "Checking knn training set size:  (1458,)\n",
      "knn RMSE:  0.167952442091\n",
      "\n",
      "Checking rf training set size:  (1458,)\n",
      "rf RMSE:  0.132003171424\n",
      "\n",
      "<type 'numpy.ndarray'> (1459,) (1459,) (1459,) (1459,)\n",
      "\n",
      "RF Stacker CV RMSE:  0.12311867718\n",
      "\n",
      "          SalePrice\n",
      "Id                 \n",
      "1461  129207.666221\n",
      "1462  171987.873184\n",
      "1463  172548.736950\n",
      "1464  204578.648897\n",
      "1465  199166.941510\n",
      "\n",
      "Linear Stacker CV RMSE:  0.110599566173\n",
      "\n",
      "          SalePrice\n",
      "Id                 \n",
      "1461  120558.833097\n",
      "1462  162124.185112\n",
      "1463  178508.650558\n",
      "1464  198781.608529\n",
      "1465  199234.023798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/scipy/linalg/basic.py:884: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver.\n",
      "  warnings.warn(mesg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "# Function to load in all models.\n",
    "def load_model(model_name, n_folds=10):\n",
    "    ''' Input the model name to be loaded, and n_folds used.\n",
    "    Returns the model that is aggregated from weights predicted from CV sets. '''\n",
    "    train = []\n",
    "    for i in range(n_folds):\n",
    "        train.append(np.loadtxt('../model_4/{}_pred_fold_{}.txt'.format(model_name, i)))\n",
    "    return train\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train = pd.read_csv('../train.csv')\n",
    "    test = pd.read_csv('../test.csv')\n",
    "    y_train = np.log1p(train.SalePrice)\n",
    "    outliers_id = np.array([523, 1298])\n",
    "    y_train = y_train.drop(outliers_id)\n",
    "\n",
    "    ### Aggregate weights to be passed into layer 1 model\n",
    "    # 1. This is Lasso predicted weights from Kfold training set\n",
    "    train_lasso = load_model('ls', 10)\n",
    "    train_lasso_folds = np.hstack((train_lasso[0], train_lasso[1], train_lasso[2], train_lasso[3], \\\n",
    "                                train_lasso[4], train_lasso[5], train_lasso[6], \\\n",
    "                                train_lasso[7], train_lasso[8], train_lasso[9]))\n",
    "    print \"\\nChecking Lasso trainin set size: \", train_lasso_folds.shape\n",
    "\n",
    "    rmse_check_1 = np.sqrt(mean_squared_error(np.log(train_lasso_folds), y_train))\n",
    "    print \"Lasso RMSE: \", rmse_check_1\n",
    "\n",
    "\n",
    "    # 2. This is Ridge predicted weights from Kfold training set\n",
    "    train_ridge = load_model('ridge', 10)\n",
    "    train_ridge_folds = np.hstack((train_ridge[0], train_ridge[1], train_ridge[2], train_ridge[3], \\\n",
    "                                train_ridge[4], train_ridge[5], train_ridge[6], \\\n",
    "                                train_ridge[7], train_ridge[8], train_ridge[9]))\n",
    "    print \"\\nChecking Ridge trainin set size: \", train_ridge_folds.shape\n",
    "\n",
    "    rmse_check_2 = np.sqrt(mean_squared_error(np.log(train_ridge_folds), y_train))\n",
    "    print \"Ridge RMSE: \", rmse_check_2\n",
    "\n",
    "\n",
    "    # 3. This is xgb predicted weights from Kfold training set\n",
    "    train_xgb = load_model('xgb', 10)\n",
    "    train_xgb_folds = np.hstack((train_xgb[0], train_xgb[1], train_xgb[2], train_xgb[3], \\\n",
    "                                train_xgb[4], train_xgb[5], train_xgb[6], \\\n",
    "                                train_xgb[7], train_xgb[8], train_xgb[9]))\n",
    "    print \"\\nChecking XGB training set size: \", train_xgb_folds.shape\n",
    "\n",
    "    rmse_check_3 = np.sqrt(mean_squared_error(np.log(train_xgb_folds), y_train))\n",
    "    print \"XGB RMSE: \", rmse_check_3\n",
    "\n",
    "\n",
    "    # 4. This is knn predicted weights from Kfold training set\n",
    "    train_knn = load_model('knn', 10)\n",
    "    train_knn_folds = np.hstack((train_knn[0], train_knn[1], train_knn[2], train_knn[3], \\\n",
    "                                train_knn[4], train_knn[5], train_knn[6], \\\n",
    "                                train_knn[7], train_knn[8], train_knn[9]))\n",
    "    print \"\\nChecking knn training set size: \", train_knn_folds.shape\n",
    "\n",
    "    rmse_check_4 = np.sqrt(mean_squared_error(np.log(train_knn_folds), y_train))\n",
    "    print \"knn RMSE: \", rmse_check_4\n",
    "\n",
    "\n",
    "    # 5. This is rf predicted weights from Kfold training set\n",
    "    train_rf = load_model('rf', 10)\n",
    "    train_rf_folds = np.hstack((train_rf[0], train_rf[1], train_rf[2], train_rf[3], \\\n",
    "                                train_rf[4], train_rf[5], train_rf[6], \\\n",
    "                                train_rf[7], train_rf[8], train_rf[9]))\n",
    "    print \"\\nChecking rf training set size: \", train_rf_folds.shape\n",
    "\n",
    "    rmse_check_5 = np.sqrt(mean_squared_error(np.log(train_rf_folds), y_train))\n",
    "    print \"rf RMSE: \", rmse_check_5\n",
    "\n",
    "    ### Read in the prediction on test set. This will be test_x (test set features)\n",
    "    xgb_pred = np.loadtxt('../model_4_test_set/xgb_test_pred.txt')\n",
    "    lasso_pred = np.loadtxt('../model_4_test_set/lasso_test_pred.txt')\n",
    "    ridge_pred = np.loadtxt('../model_4_test_set/ridge_test_pred.txt')\n",
    "    rf_pred = np.loadtxt('../model_4_test_set/rf_test_pred.txt')\n",
    "    knn_pred = np.loadtxt('../model_4_test_set/knn_test_pred.txt')\n",
    "\n",
    "\n",
    "    # Resize the ridge and knn prediction so they can fit into the stacker.\n",
    "    print '\\n', type(xgb_pred), lasso_pred.shape, ridge_pred.shape, knn_pred.shape, rf_pred.shape\n",
    "    # Stacking starts here.\n",
    "    layer_1_train_x = np.vstack((train_xgb_folds, train_lasso_folds, train_ridge_folds, train_rf_folds, train_knn_folds)).T\n",
    "    layer_1_test_x = np.vstack((xgb_pred, lasso_pred, ridge_pred, rf_pred, knn_pred)).T\n",
    "\n",
    "\n",
    "    ### Use Random Forest to do the stacking\n",
    "    rf_stack = RandomForestRegressor()\n",
    "    rf_stack_rmse = np.sqrt(-cross_val_score(rf_stack, np.log(layer_1_train_x), y_train, cv=5, scoring='neg_mean_squared_error'))\n",
    "    print \"\\nRF Stacker CV RMSE: \", (rf_stack_rmse.mean())\n",
    "\n",
    "    rf_stack.fit(np.log(layer_1_train_x), y_train)\n",
    "    rf_stack_final_pred = rf_stack.predict(layer_1_test_x)\n",
    "    df_rf_stack_final_pred = pd.DataFrame(np.exp(rf_stack_final_pred), index=test[\"Id\"], columns=[\"SalePrice\"])\n",
    "    df_rf_stack_final_pred.to_csv('submission13_rf_stack.csv', header=True, index_label='Id')\n",
    "    print \"\\n\", df_rf_stack_final_pred.head()\n",
    "\n",
    "\n",
    "    ### User Linear Regression to do the stacking\n",
    "    lr = LinearRegression()\n",
    "    lr_rmse = np.sqrt(-cross_val_score(lr, np.log(layer_1_train_x), y_train, cv=5, scoring='neg_mean_squared_error'))\n",
    "    print \"\\nLinear Stacker CV RMSE: \", (lr_rmse.mean())\n",
    "\n",
    "    lr.fit(np.log(layer_1_train_x), y_train)\n",
    "    final_pred = lr.predict(layer_1_test_x)\n",
    "    df_final_pred = pd.DataFrame(np.exp(final_pred), index=test[\"Id\"], columns=[\"SalePrice\"])\n",
    "    df_final_pred.to_csv('submission12_linear_stack.csv', header=True, index_label='Id')\n",
    "    print \"\\n\", df_final_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:  1.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Score: ', {'param_scale_pos_weight': masked_array(data = [1 1 1 1 1 1],\n",
      "             mask = [False False False False False False],\n",
      "       fill_value = ?)\n",
      ", 'rank_test_score': array([1, 2, 3, 5, 4, 6], dtype=int32), 'param_gamma': masked_array(data = [0 0 0 0 0 0],\n",
      "             mask = [False False False False False False],\n",
      "       fill_value = ?)\n",
      ", 'split6_test_score': array([-0.01421315, -0.01420057, -0.01410908, -0.0147819 , -0.0147802 ,\n",
      "       -0.01464187]), 'split7_train_score': array([-0.00992227, -0.01000877, -0.01011812, -0.007294  , -0.00757964,\n",
      "       -0.0078914 ]), 'split0_train_score': array([-0.00923204, -0.00927174, -0.00936809, -0.00685629, -0.00704583,\n",
      "       -0.00733514]), 'split2_test_score': array([-0.01028112, -0.01025563, -0.01016041, -0.01068304, -0.01059665,\n",
      "       -0.0104838 ]), 'mean_fit_time': array([ 1.42232699,  1.48049843,  1.53980062,  2.33758044,  2.1454263 ,\n",
      "        2.14090152]), 'split3_train_score': array([-0.00938455, -0.00943336, -0.00951781, -0.00677874, -0.00703061,\n",
      "       -0.00729353]), 'split6_train_score': array([-0.0096928 , -0.00973417, -0.00983788, -0.00711136, -0.00729827,\n",
      "       -0.00760094]), 'split9_test_score': array([-0.01779927, -0.01754185, -0.0174252 , -0.01876865, -0.01840546,\n",
      "       -0.01821421]), 'std_test_score': array([ 0.00367241,  0.00362262,  0.00367101,  0.0036421 ,  0.00365592,\n",
      "        0.00368966]), 'param_n_estimators': masked_array(data = [5000 5000 5000 5000 5000 5000],\n",
      "             mask = [False False False False False False],\n",
      "       fill_value = ?)\n",
      ", 'params': ({'colsample_bytree': 0.6, 'scale_pos_weight': 1, 'learning_rate': 0.01, 'min_child_weight': 6, 'n_estimators': 5000, 'subsample': 0.7, 'reg_lambda': 0.9, 'seed': 42, 'max_depth': 1, 'gamma': 0}, {'colsample_bytree': 0.6, 'scale_pos_weight': 1, 'learning_rate': 0.01, 'min_child_weight': 7, 'n_estimators': 5000, 'subsample': 0.7, 'reg_lambda': 0.9, 'seed': 42, 'max_depth': 1, 'gamma': 0}, {'colsample_bytree': 0.6, 'scale_pos_weight': 1, 'learning_rate': 0.01, 'min_child_weight': 9, 'n_estimators': 5000, 'subsample': 0.7, 'reg_lambda': 0.9, 'seed': 42, 'max_depth': 1, 'gamma': 0}, {'colsample_bytree': 0.6, 'scale_pos_weight': 1, 'learning_rate': 0.01, 'min_child_weight': 6, 'n_estimators': 5000, 'subsample': 0.7, 'reg_lambda': 0.9, 'seed': 42, 'max_depth': 2, 'gamma': 0}, {'colsample_bytree': 0.6, 'scale_pos_weight': 1, 'learning_rate': 0.01, 'min_child_weight': 7, 'n_estimators': 5000, 'subsample': 0.7, 'reg_lambda': 0.9, 'seed': 42, 'max_depth': 2, 'gamma': 0}, {'colsample_bytree': 0.6, 'scale_pos_weight': 1, 'learning_rate': 0.01, 'min_child_weight': 9, 'n_estimators': 5000, 'subsample': 0.7, 'reg_lambda': 0.9, 'seed': 42, 'max_depth': 2, 'gamma': 0}), 'split8_test_score': array([-0.00998649, -0.01014721, -0.01020662, -0.01097273, -0.01106643,\n",
      "       -0.01121931]), 'std_score_time': array([ 0.00031892,  0.00128801,  0.00209542,  0.00156349,  0.00910218,\n",
      "        0.00135691]), 'std_fit_time': array([ 0.09290162,  0.20256219,  0.21723496,  0.21700527,  0.18362433,\n",
      "        0.170425  ]), 'std_train_score': array([ 0.00034743,  0.00034945,  0.00034552,  0.00025601,  0.00024815,\n",
      "        0.00026852]), 'split4_test_score': array([-0.01988998, -0.01976668, -0.02001704, -0.02034062, -0.02045644,\n",
      "       -0.02064571]), 'param_max_depth': masked_array(data = [1 1 1 2 2 2],\n",
      "             mask = [False False False False False False],\n",
      "       fill_value = ?)\n",
      ", 'split1_train_score': array([-0.009851  , -0.0099031 , -0.01000406, -0.00730951, -0.00750509,\n",
      "       -0.00785693]), 'split2_train_score': array([-0.00981664, -0.00987585, -0.00995705, -0.00733956, -0.00756645,\n",
      "       -0.00782705]), 'param_subsample': masked_array(data = [0.7 0.7 0.7 0.7 0.7 0.7],\n",
      "             mask = [False False False False False False],\n",
      "       fill_value = ?)\n",
      ", 'param_seed': masked_array(data = [42 42 42 42 42 42],\n",
      "             mask = [False False False False False False],\n",
      "       fill_value = ?)\n",
      ", 'split4_train_score': array([-0.00889352, -0.00894122, -0.00902889, -0.00668224, -0.00687816,\n",
      "       -0.00708162]), 'mean_score_time': array([ 0.00377066,  0.00474839,  0.00567636,  0.00640807,  0.01196868,\n",
      "        0.00647752]), 'param_colsample_bytree': masked_array(data = [0.6 0.6 0.6 0.6 0.6 0.6],\n",
      "             mask = [False False False False False False],\n",
      "       fill_value = ?)\n",
      ", 'split9_train_score': array([-0.00910865, -0.00918362, -0.0093019 , -0.00682304, -0.00708603,\n",
      "       -0.00734631]), 'split5_test_score': array([-0.01177004, -0.01187463, -0.0120345 , -0.01278171, -0.01287901,\n",
      "       -0.01299686]), 'mean_train_score': array([-0.00954549, -0.00959933, -0.00969079, -0.00707625, -0.00728932,\n",
      "       -0.00756559]), 'split8_train_score': array([-0.00989827, -0.00993857, -0.00998731, -0.00742628, -0.0075486 ,\n",
      "       -0.00777612]), 'split7_test_score': array([-0.00856001, -0.00841262, -0.00836082, -0.00965924, -0.00923463,\n",
      "       -0.00915784]), 'split0_test_score': array([-0.01583602, -0.01593034, -0.01609488, -0.01708636, -0.01713355,\n",
      "       -0.01724197]), 'mean_test_score': array([-0.01325047, -0.01325835, -0.01333181, -0.01405283, -0.01402636,\n",
      "       -0.01407331]), 'param_reg_lambda': masked_array(data = [0.9 0.9 0.9 0.9 0.9 0.9],\n",
      "             mask = [False False False False False False],\n",
      "       fill_value = ?)\n",
      ", 'param_min_child_weight': masked_array(data = [6 7 9 6 7 9],\n",
      "             mask = [False False False False False False],\n",
      "       fill_value = ?)\n",
      ", 'split3_test_score': array([-0.01493011, -0.01508669, -0.01540163, -0.01555463, -0.01563541,\n",
      "       -0.01599017]), 'split5_train_score': array([-0.00965516, -0.00970292, -0.00978681, -0.00714146, -0.0073545 ,\n",
      "       -0.00764684]), 'param_learning_rate': masked_array(data = [0.01 0.01 0.01 0.01 0.01 0.01],\n",
      "             mask = [False False False False False False],\n",
      "       fill_value = ?)\n",
      ", 'split1_test_score': array([-0.00924728, -0.00937533, -0.00951461, -0.00991061, -0.01008553,\n",
      "       -0.01015022])})\n",
      "('mean_test_score: ', array([-0.01325047, -0.01325835, -0.01333181, -0.01405283, -0.01402636,\n",
      "       -0.01407331]))\n",
      "('Average mean_test_score: ', -0.013665522722586627)\n",
      "{'colsample_bytree': 0.6, 'scale_pos_weight': 1, 'learning_rate': 0.01, 'min_child_weight': 6, 'n_estimators': 5000, 'subsample': 0.7, 'reg_lambda': 0.9, 'seed': 42, 'max_depth': 1, 'gamma': 0}\n",
      "\n",
      "(-0.013250467492746004, 0.1151106749730276)\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Grid Search with XGBoost. Tuning max_depth & min_child_weight\n",
    "xgb_model = XGBRegressor()\n",
    "xgb_params = {'max_depth': [1], \\\n",
    "              'subsample': [0.7], \\\n",
    "              'n_estimators': [5000], \\\n",
    "              'learning_rate': [0.01], \\\n",
    "              'colsample_bytree': [0.6], \\\n",
    "              'min_child_weight': [6], \\\n",
    "              'gamma': [0], \\\n",
    "              'scale_pos_weight': [1], \\\n",
    "              'reg_lambda':[0.9], \\\n",
    "              'seed': [42]\n",
    "             }\n",
    "### Tried hyperparameters#########\n",
    "# max_depth = [2, 3, 6, 9]\n",
    "# n_estimators = [500]\n",
    "# min_child_weight = [1.25, 3, 5, 7, 9]\n",
    "##################################\n",
    "\n",
    "xgb_grid = GridSearchCV(xgb_model, xgb_params, cv=10, verbose=1, scoring='neg_mean_squared_error')\n",
    "xgb_grid.fit(np.log(layer_1_train_x), y_train)\n",
    "print(\"Score: \", xgb_grid.cv_results_)\n",
    "print(\"mean_test_score: \", xgb_grid.cv_results_['mean_test_score'])\n",
    "mean_test_score = xgb_grid.cv_results_['mean_test_score'].mean()\n",
    "print(\"Average mean_test_score: \", mean_test_score)\n",
    "\n",
    "\n",
    "### Prediction from stacker is here.\n",
    "xgb_stack_final_pred = xgb_grid.predict(layer_1_test_x)\n",
    "df_xgb_stack_final_pred = pd.DataFrame(np.exp(xgb_stack_final_pred), index=test[\"Id\"], columns=[\"SalePrice\"])\n",
    "df_xgb_stack_final_pred.to_csv('submission14_xgb_stack.csv', header=True, index_label='Id')\n",
    "\n",
    "### Print the best param and CV score here.\n",
    "print(xgb_grid.best_params_)\n",
    "print '\\n', (xgb_grid.best_score_, np.sqrt(-xgb_grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
